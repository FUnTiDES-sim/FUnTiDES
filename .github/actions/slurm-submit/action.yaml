name: slurm-submit
description: Submit and wait for a Slurm job
inputs:
  job-name:
    required: true
  partition:
    required: true
  account:
    required: false
    default: "co2su"
  time:
    required: false
    default: 00:30:00
  cpus:
    required: false
    default: "1"
  gpus:
    required: false
    default: "1"
  run-script:
    required: true
    description: Multi-line bash payload executed inside the job
runs:
  using: composite
  steps:
    - shell: bash
      run: |
        # Source bashrc
        if [ -f "~/.bashrc" ]; then
          echo "Sourcing ~/.bashrc"
          source "~/.bashrc"
        fi

        # Create Slurm job script
        set -euo pipefail

        cat > job.slurm <<'EOF'
        #!/bin/bash
        #SBATCH -J ${{ inputs.job-name }}
        #SBATCH -o slurm-%x-%j.out
        #SBATCH -e slurm-%x-%j.err
        #SBATCH -p ${{ inputs.partition }}
        #SBATCH -A ${{ inputs.account }}
        #SBATCH -t ${{ inputs.time }}
        #SBATCH -c ${{ inputs.cpus }}
        #SBATCH --gres=gpu:${{ inputs.gpus }}

        echo "Node: $(hostname)"
        echo "Start: $(date -Is)"
        EOF

        cat >> job.slurm <<'EOF'
        ${{ inputs.run-script }}
        EOF

        cat >> job.slurm <<'EOF'
        STATUS=$?
        echo "End: $(date -Is) (exit=$STATUS)"
        exit $STATUS
        EOF

        echo "Submitting Slurm job..."
        cat job.slurm

        # Assume current directory is shared/visible on maple-1; submit directly there
        set +e
        REMOTE_HOST="maple-1"
        LOCAL_DIR="$(pwd)"

        if [ ! -f job.slurm ]; then
          echo "job.slurm not found in ${LOCAL_DIR}"
          exit 1
        fi

        echo "Submitting Slurm job on ${REMOTE_HOST} from shared dir ${LOCAL_DIR}..."
        ssh -o BatchMode=yes "${REMOTE_HOST}" "cd '${LOCAL_DIR}' && \
          export SLURM_PARTITION='${{ inputs.partition }}' \
                 DISCRETIZATION='${DISCRETIZATION:-${{ env.DISCRETIZATION }}}' \
                 PYWRAP='${PYWRAP:-${{ env.PYWRAP }}}' \
                 PROGRAMMING_MODEL='${PROGRAMMING_MODEL:-${{ env.PROGRAMMING_MODEL }}}' && \
          echo '  Partition      = '\"\$SLURM_PARTITION\" && \
          echo '  Discretization = '\"\$DISCRETIZATION\" && \
          echo '  Pywrap         = '\"\$PYWRAP\" && \
          echo '  Model          = '\"\$PROGRAMMING_MODEL\" && \
          sbatch --export=SLURM_PARTITION,DISCRETIZATION,PYWRAP,PROGRAMMING_MODEL --wait job.slurm"
        SBATCH_STATUS=$?
        set -e
        echo "Tail of output:"
        tail -n 200 slurm-*.out || true

        echo "Tail of error:"
        tail -n 200 slurm-*.err || true

        # Propagate sbatch (job) exit code
        exit $SBATCH_STATUS